{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Baseline Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marinand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy: 0.4191\n"
     ]
    }
   ],
   "source": [
    "# Original model parameters but training data set reduced to 10% volume\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, models, layers, callbacks\n",
    "import numpy as np\n",
    "\n",
    "# Reduced training data: 6,000 samples (randomly sampled from the original training set)\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Reduce training set size\n",
    "reduced_train_images = train_images[:6000]\n",
    "reduced_train_labels = train_labels[:6000]\n",
    "\n",
    "# Define baseline parameters\n",
    "baseline_params = {\n",
    "    \"epochs\": 2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"filters_layer1\": 32,\n",
    "    \"filters_layer2\": 64,\n",
    "    \"dense_neurons\": 64,\n",
    "    \"dropout\": None,\n",
    "    \"batch_size\": 64\n",
    "}\n",
    "\n",
    "# Store results in a dictionary\n",
    "results = {}\n",
    "\n",
    "# Baseline Model\n",
    "print(\"Running Baseline Model...\")\n",
    "baseline_model = models.Sequential()\n",
    "baseline_model.add(layers.Conv2D(baseline_params[\"filters_layer1\"], (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "baseline_model.add(layers.MaxPooling2D((2, 2)))\n",
    "baseline_model.add(layers.Conv2D(baseline_params[\"filters_layer2\"], (3, 3), activation='relu'))\n",
    "baseline_model.add(layers.MaxPooling2D((2, 2)))\n",
    "baseline_model.add(layers.Conv2D(baseline_params[\"filters_layer2\"], (3, 3), activation='relu'))\n",
    "baseline_model.add(layers.Flatten())\n",
    "baseline_model.add(layers.Dense(baseline_params[\"dense_neurons\"], activation='relu'))\n",
    "if baseline_params[\"dropout\"]:\n",
    "    baseline_model.add(layers.Dropout(baseline_params[\"dropout\"]))\n",
    "baseline_model.add(layers.Dense(10))\n",
    "\n",
    "# Compile baseline model\n",
    "baseline_optimizer = tf.keras.optimizers.Adam(learning_rate=baseline_params[\"learning_rate\"])\n",
    "baseline_model.compile(optimizer=baseline_optimizer,\n",
    "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "baseline_history = baseline_model.fit(\n",
    "    reduced_train_images,\n",
    "    reduced_train_labels,\n",
    "    epochs=baseline_params[\"epochs\"],\n",
    "    batch_size=baseline_params[\"batch_size\"],\n",
    "    validation_data=(test_images, test_labels),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate baseline model\n",
    "baseline_test_loss, baseline_test_acc = baseline_model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "# Store baseline results\n",
    "results[\"baseline\"] = {\n",
    "    \"parameters\": baseline_params,\n",
    "    \"test_accuracy\": baseline_test_acc\n",
    "}\n",
    "print(f\"Baseline Test Accuracy: {baseline_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block to loop through different parameter combinations and store results from model performance\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, models, layers, callbacks\n",
    "import numpy as np\n",
    "\n",
    "# Reduced training data: 6,000 samples (randomly sampled from the original training set)\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Reduce training set size\n",
    "reduced_train_images = train_images[:6000]\n",
    "reduced_train_labels = train_labels[:6000]\n",
    "\n",
    "# Parameter combinations to test\n",
    "epochs_list = [5]\n",
    "learning_rate_list = [0.001, 0.0001]\n",
    "filters_layer1_list = [32, 64]\n",
    "filters_layer2_list = [64, 128]\n",
    "dense_neurons_list = [64, 128]\n",
    "dropout_list = [None, 0.3]\n",
    "batch_size_list = [32, 64]\n",
    "\n",
    "# Generate all parameter combinations\n",
    "from itertools import product\n",
    "param_combinations = list(product(epochs_list, learning_rate_list, filters_layer1_list,\n",
    "                                   filters_layer2_list, dense_neurons_list, dropout_list, batch_size_list))\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Initialize results dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Loop through parameter combinations\n",
    "for i, params in enumerate(param_combinations):\n",
    "    epochs, learning_rate, filters_layer1, filters_layer2, dense_neurons, dropout, batch_size = params\n",
    "    print(f\"Testing combination {i + 1}/{len(param_combinations)}: {params}\")\n",
    "\n",
    "    # Build model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters_layer1, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters_layer2, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters_layer2, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_neurons, activation='relu'))\n",
    "    if dropout:\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(10))\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        reduced_train_images,\n",
    "        reduced_train_labels,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test_images, test_labels),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "    # Store results\n",
    "    results[i] = {\n",
    "        \"parameters\": {\n",
    "            \"epochs\": epochs,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"filters_layer1\": filters_layer1,\n",
    "            \"filters_layer2\": filters_layer2,\n",
    "            \"dense_neurons\": dense_neurons,\n",
    "            \"dropout\": dropout,\n",
    "            \"batch_size\": batch_size\n",
    "        },\n",
    "        \"test_accuracy\": test_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results (Sorted by Test Accuracy):\n",
      "Combination\tTest Accuracy\tParameters\n",
      "28\t\t0.5402\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "0\t\t0.4810\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "4\t\t0.3662\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "20\t\t0.3654\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "8\t\t0.3650\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "29\t\t0.3537\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "24\t\t0.3504\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "12\t\t0.3502\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "30\t\t0.3460\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "18\t\t0.3443\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "9\t\t0.3421\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "13\t\t0.3420\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "1\t\t0.3400\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "5\t\t0.3387\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "22\t\t0.3370\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "31\t\t0.3366\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "15\t\t0.3224\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "6\t\t0.3205\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "16\t\t0.3128\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "11\t\t0.3105\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "10\t\t0.3101\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "23\t\t0.3073\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "21\t\t0.3051\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "14\t\t0.3025\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "25\t\t0.3013\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "17\t\t0.2887\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "52\t\t0.2880\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "56\t\t0.2877\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "44\t\t0.2862\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "26\t\t0.2839\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "36\t\t0.2755\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "19\t\t0.2754\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "62\t\t0.2730\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "58\t\t0.2723\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "2\t\t0.2711\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "60\t\t0.2702\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "45\t\t0.2691\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "41\t\t0.2659\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "27\t\t0.2631\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "55\t\t0.2558\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "7\t\t0.2557\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "54\t\t0.2538\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "42\t\t0.2535\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "38\t\t0.2534\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "32\t\t0.2528\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "40\t\t0.2500\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "48\t\t0.2497\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 32\n",
      "46\t\t0.2490\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "53\t\t0.2431\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "63\t\t0.2430\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "47\t\t0.2401\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "57\t\t0.2314\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "39\t\t0.2286\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "43\t\t0.2263\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "50\t\t0.2255\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "3\t\t0.2223\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "37\t\t0.2100\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "61\t\t0.2086\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 128\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "34\t\t0.2083\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 32\n",
      "51\t\t0.2037\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "49\t\t0.1918\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n",
      "59\t\t0.1770\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 64\n",
      "\t\t\tfilters_layer2: 128\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "35\t\t0.1761\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: 0.3\n",
      "\t\t\tbatch_size: 64\n",
      "33\t\t0.1603\n",
      "\t\t\tepochs: 5\n",
      "\t\t\tlearning_rate: 0.0001\n",
      "\t\t\tfilters_layer1: 32\n",
      "\t\t\tfilters_layer2: 64\n",
      "\t\t\tdense_neurons: 64\n",
      "\t\t\tdropout: None\n",
      "\t\t\tbatch_size: 64\n"
     ]
    }
   ],
   "source": [
    "# Sort results from loop above by test_accuracy in descending order\n",
    "sorted_results = sorted(results.items(), key=lambda item: item[1]['test_accuracy'], reverse=True)\n",
    "\n",
    "# Print sorted results as a table\n",
    "print(\"\\nFinal Results (Sorted by Test Accuracy):\")\n",
    "print(\"Combination\\tTest Accuracy\\tParameters\")\n",
    "for key, value in sorted_results:\n",
    "    params = value['parameters']\n",
    "    formatted_params = \"\\n\\t\\t\\t\".join([f\"{k}: {v}\" for k, v in params.items()])\n",
    "    print(f\"{key}\\t\\t{value['test_accuracy']:.4f}\\n\\t\\t\\t{formatted_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model with best parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marinand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.1970 - loss: 2.1366 - val_accuracy: 0.3333 - val_loss: 1.8523\n",
      "Epoch 2/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - accuracy: 0.3617 - loss: 1.7130 - val_accuracy: 0.4098 - val_loss: 1.6191\n",
      "Epoch 3/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.4486 - loss: 1.5153 - val_accuracy: 0.4807 - val_loss: 1.4353\n",
      "Epoch 4/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - accuracy: 0.5251 - loss: 1.3325 - val_accuracy: 0.4984 - val_loss: 1.3995\n",
      "Epoch 5/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.5614 - loss: 1.2432 - val_accuracy: 0.5072 - val_loss: 1.3598\n",
      "Epoch 6/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.6013 - loss: 1.1071 - val_accuracy: 0.5317 - val_loss: 1.3460\n",
      "Epoch 7/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.6469 - loss: 1.0166 - val_accuracy: 0.5436 - val_loss: 1.3051\n",
      "Epoch 8/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - accuracy: 0.6799 - loss: 0.9227 - val_accuracy: 0.5646 - val_loss: 1.2731\n",
      "Epoch 9/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.7070 - loss: 0.8089 - val_accuracy: 0.5706 - val_loss: 1.3025\n",
      "Epoch 10/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 74ms/step - accuracy: 0.7641 - loss: 0.6791 - val_accuracy: 0.5595 - val_loss: 1.3534\n",
      "Epoch 11/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.7927 - loss: 0.6034 - val_accuracy: 0.5715 - val_loss: 1.3795\n",
      "Epoch 12/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 75ms/step - accuracy: 0.8161 - loss: 0.5201 - val_accuracy: 0.5571 - val_loss: 1.5187\n",
      "Epoch 13/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 72ms/step - accuracy: 0.8618 - loss: 0.4158 - val_accuracy: 0.5740 - val_loss: 1.5493\n",
      "Epoch 14/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.8931 - loss: 0.3170 - val_accuracy: 0.5739 - val_loss: 1.7009\n",
      "Epoch 15/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - accuracy: 0.9309 - loss: 0.2223 - val_accuracy: 0.5709 - val_loss: 1.7738\n",
      "Epoch 16/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - accuracy: 0.9436 - loss: 0.1793 - val_accuracy: 0.5718 - val_loss: 1.9166\n",
      "313/313 - 3s - 10ms/step - accuracy: 0.5740 - loss: 1.5493\n",
      "Test Accuracy: 0.5740\n"
     ]
    }
   ],
   "source": [
    "# Model implementation using the best performing parameters\n",
    "# Number of epochs increased to 20\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, models, layers, callbacks\n",
    "import numpy as np\n",
    "\n",
    "# Reduced training data: 6,000 samples (randomly sampled from the original training set)\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Reduce training set size\n",
    "reduced_train_images = train_images[:6000]\n",
    "reduced_train_labels = train_labels[:6000]\n",
    "\n",
    "# Define the parameters you want to use\n",
    "best_params = {\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"filters_layer1\": 64,\n",
    "    \"filters_layer2\": 128,\n",
    "    \"dense_neurons\": 128,\n",
    "    \"dropout\": None,\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "# Build model with the chosen parameters\n",
    "print(\"Running model with best parameters...\")\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(best_params[\"filters_layer1\"], (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(best_params[\"filters_layer2\"], (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(best_params[\"filters_layer2\"], (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(best_params[\"dense_neurons\"], activation='relu'))\n",
    "if best_params[\"dropout\"]:\n",
    "    model.add(layers.Dropout(best_params[\"dropout\"]))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=best_params[\"learning_rate\"])\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    reduced_train_images,\n",
    "    reduced_train_labels,\n",
    "    epochs=best_params[\"epochs\"],\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    validation_data=(test_images, test_labels),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1  # Change to 1 or 2 to see progress\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
